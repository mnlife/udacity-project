{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03e5dfbc50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(range(10))\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # Create the encoder 创建编码器\n",
    "    \n",
    "    #print(\"x\",x)\n",
    "    \n",
    "    output = encoder.transform(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #x, y, z = image_shape\n",
    "    return tf.placeholder(tf.float32, shape = (None, image_shape[0], image_shape[1], image_shape[2]), name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = (None, n_classes), name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = None, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import problem_unittests as tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, max_pool_padding = \"SAME\"):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    #W = tf.get_variable(\"W\", [a, b, z, conv_num_outputs], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W = tf.Variable(tf.truncated_normal([int(conv_ksize[0]), int(conv_ksize[1]), int(x_tensor.shape[3]), int(conv_num_outputs)], stddev = 0.05))\n",
    "    #b = tf.get_variable(\"b\", [4, 4, 3, 8], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    Z = tf.nn.conv2d(x_tensor, W, strides = [1, conv_strides[0], conv_strides[1], 1], padding = \"SAME\")\n",
    "    A = tf.nn.relu(tf.nn.bias_add(Z, b))\n",
    "    P1 = tf.nn.max_pool(A, ksize = [1, pool_ksize[0], pool_ksize[1], 1], strides = [1, pool_strides[0], pool_strides[1], 1], padding = max_pool_padding)\n",
    "    return P1\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn = tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn = None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    Z1 = conv2d_maxpool(x, 48, (5,5), (1,1), (3,3), (3,3), max_pool_padding = \"SAME\")\n",
    "    Z2 = conv2d_maxpool(Z1, 128, (3,3), (1,1), (2,2), (2,2), max_pool_padding = \"SAME\")\n",
    "    Z2 = tf.nn.dropout(Z2, keep_prob)\n",
    "    Z3 = conv2d_maxpool(Z2, 192, (3,3), (1,1), (2,2), (2,2), max_pool_padding = \"VALID\")\n",
    "    Z4 = conv2d_maxpool(Z3, 192, (3,3), (1,1), (2,2), (2,2), max_pool_padding = \"SAME\")\n",
    "    Z4 = tf.nn.dropout(Z4, keep_prob)\n",
    "    Z5 = conv2d_maxpool(Z4, 128, (3,3), (1,1), (2,2), (2,2), max_pool_padding = \"VALID\")\n",
    "    A5 = tf.nn.dropout(Z5, keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    A5 = flatten(A5)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    Z6 = fully_conn(A5, 256)\n",
    "    A6 = tf.nn.dropout(Z6, keep_prob)\n",
    "    Z7 = fully_conn(A6, 128)\n",
    "    A7 = tf.nn.dropout(Z7, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    A8 = output(A7, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return A8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    #label_batch.dtype = \"int\"\n",
    "    #print(label_batch[0:2])\n",
    "    # TODO: Implement Function\n",
    "    #X_train = normalize(feature_batch)\n",
    "    #print(\"X_train.shape = \"+str(X_train.shape))\n",
    "    #label = label_batch*10\n",
    "    #label_batch = label_batch.astype(int)\n",
    "    #print (\"label_batch=\"+str(label_batch.shape))\n",
    "    #y_train = one_hot_encode(label_batch)\n",
    "    #y_train = label_batch\n",
    "    \n",
    "    \n",
    "    #x_tensor = neural_net_image_input((X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
    "    \n",
    "    #print (\"y_train.shape=\"+str(y_train.shape))\n",
    "    \n",
    "    #y_tensor = neural_net_label_input(y_train.shape[1])\n",
    "    #print(\"x_tensor.shape = \"+str(x_tensor.shape))\n",
    "    #print(\"y_tensor.shape = \"+str(y_tensor.shape))\n",
    "    #print(\"label_batch.shape = \"+str(label_batch.shape))\n",
    "    #print(\"feature_batch.shape = \"+str(feature_batch.shape))\n",
    "    #keep_prob = neural_net_keep_prob_input()\n",
    "    \n",
    "    #logits = conv_net(x_tensor, keep_prob)\n",
    "    \n",
    "    #logits = tf.identity(logits, name='logits')\n",
    "    \n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_train))\n",
    "    \n",
    "    #correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    \n",
    "    #init = tf.global_variables_initializer()\n",
    "    #with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        #sess.run(init)\n",
    "    \n",
    "    temp_cost = session.run([optimizer, cost], feed_dict = {x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #Z = conv_net(feature_batch, 1)\n",
    "    #y_pred = tf.nn.softmax(Z)\n",
    "    #predict_op = tf.argmax(y_pred, 1)\n",
    "    #correct_prediction = tf.equal(predict_op, tf.argmax(label_batch, 1))\n",
    "        \n",
    "    # Calculate accuracy on the test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    #accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    train_acc = sess.run(accuracy, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} train Accuracy: {:.6f} Validation Accuracy: {:.6f} '.format(\n",
    "                loss, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 128\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3007 train Accuracy: 0.125000 Validation Accuracy: 0.105000 \n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2626 train Accuracy: 0.125000 Validation Accuracy: 0.174200 \n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2218 train Accuracy: 0.200000 Validation Accuracy: 0.175600 \n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1826 train Accuracy: 0.150000 Validation Accuracy: 0.185400 \n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1410 train Accuracy: 0.150000 Validation Accuracy: 0.181600 \n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0947 train Accuracy: 0.175000 Validation Accuracy: 0.224000 \n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0944 train Accuracy: 0.175000 Validation Accuracy: 0.203200 \n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9614 train Accuracy: 0.200000 Validation Accuracy: 0.273000 \n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.9783 train Accuracy: 0.225000 Validation Accuracy: 0.288600 \n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.9342 train Accuracy: 0.200000 Validation Accuracy: 0.310600 \n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7640 train Accuracy: 0.375000 Validation Accuracy: 0.317800 \n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.7408 train Accuracy: 0.325000 Validation Accuracy: 0.341200 \n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5976 train Accuracy: 0.425000 Validation Accuracy: 0.357600 \n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5270 train Accuracy: 0.450000 Validation Accuracy: 0.387000 \n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.4279 train Accuracy: 0.475000 Validation Accuracy: 0.389400 \n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.3751 train Accuracy: 0.425000 Validation Accuracy: 0.404400 \n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2898 train Accuracy: 0.500000 Validation Accuracy: 0.417600 \n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2354 train Accuracy: 0.475000 Validation Accuracy: 0.434600 \n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1388 train Accuracy: 0.625000 Validation Accuracy: 0.440400 \n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.0791 train Accuracy: 0.650000 Validation Accuracy: 0.477600 \n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.0633 train Accuracy: 0.625000 Validation Accuracy: 0.456600 \n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.9200 train Accuracy: 0.700000 Validation Accuracy: 0.494200 \n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.9464 train Accuracy: 0.675000 Validation Accuracy: 0.468800 \n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.7897 train Accuracy: 0.750000 Validation Accuracy: 0.514400 \n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.8747 train Accuracy: 0.775000 Validation Accuracy: 0.485400 \n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.7497 train Accuracy: 0.775000 Validation Accuracy: 0.506800 \n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.7070 train Accuracy: 0.800000 Validation Accuracy: 0.532200 \n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.6589 train Accuracy: 0.725000 Validation Accuracy: 0.554200 \n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.6300 train Accuracy: 0.700000 Validation Accuracy: 0.535200 \n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.6036 train Accuracy: 0.750000 Validation Accuracy: 0.545400 \n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.6018 train Accuracy: 0.775000 Validation Accuracy: 0.555800 \n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.5635 train Accuracy: 0.800000 Validation Accuracy: 0.559800 \n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.5292 train Accuracy: 0.850000 Validation Accuracy: 0.544200 \n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.5321 train Accuracy: 0.800000 Validation Accuracy: 0.572200 \n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.4990 train Accuracy: 0.800000 Validation Accuracy: 0.582200 \n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.4860 train Accuracy: 0.775000 Validation Accuracy: 0.576600 \n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.4263 train Accuracy: 0.875000 Validation Accuracy: 0.591600 \n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.4475 train Accuracy: 0.875000 Validation Accuracy: 0.597200 \n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.4298 train Accuracy: 0.800000 Validation Accuracy: 0.585400 \n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.3811 train Accuracy: 0.875000 Validation Accuracy: 0.590400 \n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.3651 train Accuracy: 0.850000 Validation Accuracy: 0.595600 \n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.3645 train Accuracy: 0.925000 Validation Accuracy: 0.604200 \n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.3439 train Accuracy: 0.900000 Validation Accuracy: 0.608400 \n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.3738 train Accuracy: 0.850000 Validation Accuracy: 0.593200 \n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.3471 train Accuracy: 0.875000 Validation Accuracy: 0.606000 \n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.2878 train Accuracy: 0.850000 Validation Accuracy: 0.608400 \n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.2941 train Accuracy: 0.925000 Validation Accuracy: 0.598800 \n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.2825 train Accuracy: 0.900000 Validation Accuracy: 0.623000 \n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.2856 train Accuracy: 0.875000 Validation Accuracy: 0.612600 \n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.2735 train Accuracy: 0.875000 Validation Accuracy: 0.620200 \n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.2660 train Accuracy: 0.850000 Validation Accuracy: 0.616800 \n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.2534 train Accuracy: 0.900000 Validation Accuracy: 0.605000 \n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.2677 train Accuracy: 0.850000 Validation Accuracy: 0.633000 \n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.2238 train Accuracy: 0.900000 Validation Accuracy: 0.623400 \n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.2168 train Accuracy: 0.925000 Validation Accuracy: 0.621800 \n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.2133 train Accuracy: 0.950000 Validation Accuracy: 0.628200 \n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.2231 train Accuracy: 0.925000 Validation Accuracy: 0.604000 \n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1937 train Accuracy: 0.950000 Validation Accuracy: 0.614800 \n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.1930 train Accuracy: 0.975000 Validation Accuracy: 0.627000 \n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.2076 train Accuracy: 0.950000 Validation Accuracy: 0.601000 \n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.2251 train Accuracy: 0.925000 Validation Accuracy: 0.605400 \n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.1898 train Accuracy: 0.950000 Validation Accuracy: 0.633400 \n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1702 train Accuracy: 0.950000 Validation Accuracy: 0.637200 \n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1616 train Accuracy: 0.950000 Validation Accuracy: 0.639400 \n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.1571 train Accuracy: 0.975000 Validation Accuracy: 0.639200 \n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.1725 train Accuracy: 0.975000 Validation Accuracy: 0.638400 \n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1407 train Accuracy: 0.975000 Validation Accuracy: 0.646200 \n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.1499 train Accuracy: 0.975000 Validation Accuracy: 0.644000 \n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.1391 train Accuracy: 0.950000 Validation Accuracy: 0.631800 \n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.1677 train Accuracy: 0.950000 Validation Accuracy: 0.641800 \n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.1455 train Accuracy: 0.950000 Validation Accuracy: 0.629400 \n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.1121 train Accuracy: 1.000000 Validation Accuracy: 0.640600 \n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.1113 train Accuracy: 0.975000 Validation Accuracy: 0.642000 \n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.1064 train Accuracy: 0.975000 Validation Accuracy: 0.643800 \n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.1103 train Accuracy: 0.975000 Validation Accuracy: 0.648800 \n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.1186 train Accuracy: 0.950000 Validation Accuracy: 0.650800 \n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0986 train Accuracy: 0.975000 Validation Accuracy: 0.645400 \n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.1074 train Accuracy: 0.975000 Validation Accuracy: 0.650600 \n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.1000 train Accuracy: 0.975000 Validation Accuracy: 0.656400 \n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0836 train Accuracy: 1.000000 Validation Accuracy: 0.659400 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0779 train Accuracy: 1.000000 Validation Accuracy: 0.649200 \n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0884 train Accuracy: 0.975000 Validation Accuracy: 0.647600 \n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0937 train Accuracy: 0.950000 Validation Accuracy: 0.634800 \n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0664 train Accuracy: 1.000000 Validation Accuracy: 0.657600 \n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0819 train Accuracy: 0.975000 Validation Accuracy: 0.656000 \n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0849 train Accuracy: 0.975000 Validation Accuracy: 0.645000 \n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0749 train Accuracy: 1.000000 Validation Accuracy: 0.656200 \n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0620 train Accuracy: 1.000000 Validation Accuracy: 0.661200 \n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0668 train Accuracy: 0.975000 Validation Accuracy: 0.655200 \n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0635 train Accuracy: 1.000000 Validation Accuracy: 0.658200 \n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0788 train Accuracy: 0.975000 Validation Accuracy: 0.658600 \n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0590 train Accuracy: 1.000000 Validation Accuracy: 0.653600 \n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0557 train Accuracy: 0.975000 Validation Accuracy: 0.655400 \n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0570 train Accuracy: 1.000000 Validation Accuracy: 0.659400 \n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0493 train Accuracy: 1.000000 Validation Accuracy: 0.664000 \n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0503 train Accuracy: 1.000000 Validation Accuracy: 0.655000 \n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0521 train Accuracy: 1.000000 Validation Accuracy: 0.651800 \n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0551 train Accuracy: 1.000000 Validation Accuracy: 0.651200 \n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0433 train Accuracy: 1.000000 Validation Accuracy: 0.658200 \n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0388 train Accuracy: 1.000000 Validation Accuracy: 0.650200 \n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.0439 train Accuracy: 1.000000 Validation Accuracy: 0.659200 \n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.0527 train Accuracy: 1.000000 Validation Accuracy: 0.647600 \n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.0568 train Accuracy: 1.000000 Validation Accuracy: 0.645800 \n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.0477 train Accuracy: 1.000000 Validation Accuracy: 0.649600 \n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.0479 train Accuracy: 1.000000 Validation Accuracy: 0.660000 \n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.0384 train Accuracy: 1.000000 Validation Accuracy: 0.663600 \n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.0291 train Accuracy: 1.000000 Validation Accuracy: 0.671000 \n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.0608 train Accuracy: 1.000000 Validation Accuracy: 0.653600 \n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.0503 train Accuracy: 1.000000 Validation Accuracy: 0.644800 \n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.0483 train Accuracy: 0.975000 Validation Accuracy: 0.645800 \n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.0365 train Accuracy: 1.000000 Validation Accuracy: 0.654000 \n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.0318 train Accuracy: 1.000000 Validation Accuracy: 0.660400 \n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.0357 train Accuracy: 1.000000 Validation Accuracy: 0.661200 \n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.0251 train Accuracy: 1.000000 Validation Accuracy: 0.656000 \n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.0241 train Accuracy: 1.000000 Validation Accuracy: 0.665200 \n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.0363 train Accuracy: 1.000000 Validation Accuracy: 0.655200 \n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.0326 train Accuracy: 1.000000 Validation Accuracy: 0.654400 \n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.0240 train Accuracy: 1.000000 Validation Accuracy: 0.634600 \n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.0204 train Accuracy: 1.000000 Validation Accuracy: 0.650600 \n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.0202 train Accuracy: 1.000000 Validation Accuracy: 0.657400 \n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.0228 train Accuracy: 1.000000 Validation Accuracy: 0.663400 \n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.0179 train Accuracy: 1.000000 Validation Accuracy: 0.666800 \n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.0246 train Accuracy: 1.000000 Validation Accuracy: 0.656400 \n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.0210 train Accuracy: 1.000000 Validation Accuracy: 0.657400 \n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.0254 train Accuracy: 1.000000 Validation Accuracy: 0.655800 \n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.0199 train Accuracy: 1.000000 Validation Accuracy: 0.655400 \n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.0125 train Accuracy: 1.000000 Validation Accuracy: 0.672200 \n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.0185 train Accuracy: 1.000000 Validation Accuracy: 0.655000 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            #print(\"batch_features:\"+str(batch_features))\n",
    "            #print(\"batch_labels:\"+str(batch_labels))\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features/1.0, batch_labels/1.0)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2984 train Accuracy: 0.100000 Validation Accuracy: 0.110000 \n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2764 train Accuracy: 0.175000 Validation Accuracy: 0.181000 \n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1808 train Accuracy: 0.175000 Validation Accuracy: 0.158400 \n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.0869 train Accuracy: 0.150000 Validation Accuracy: 0.187600 \n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.9723 train Accuracy: 0.300000 Validation Accuracy: 0.194000 \n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1898 train Accuracy: 0.225000 Validation Accuracy: 0.210400 \n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9229 train Accuracy: 0.325000 Validation Accuracy: 0.183000 \n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.7700 train Accuracy: 0.350000 Validation Accuracy: 0.218200 \n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.8505 train Accuracy: 0.250000 Validation Accuracy: 0.246600 \n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.9168 train Accuracy: 0.275000 Validation Accuracy: 0.234400 \n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.8739 train Accuracy: 0.250000 Validation Accuracy: 0.315200 \n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.7789 train Accuracy: 0.400000 Validation Accuracy: 0.332600 \n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.5079 train Accuracy: 0.500000 Validation Accuracy: 0.315200 \n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.6546 train Accuracy: 0.300000 Validation Accuracy: 0.342600 \n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7230 train Accuracy: 0.375000 Validation Accuracy: 0.341200 \n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.7340 train Accuracy: 0.375000 Validation Accuracy: 0.409200 \n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.6008 train Accuracy: 0.450000 Validation Accuracy: 0.385200 \n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.4237 train Accuracy: 0.500000 Validation Accuracy: 0.376400 \n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.5387 train Accuracy: 0.350000 Validation Accuracy: 0.443800 \n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.5055 train Accuracy: 0.475000 Validation Accuracy: 0.432400 \n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.4565 train Accuracy: 0.475000 Validation Accuracy: 0.452200 \n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.4362 train Accuracy: 0.500000 Validation Accuracy: 0.454200 \n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.2180 train Accuracy: 0.675000 Validation Accuracy: 0.472200 \n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.3889 train Accuracy: 0.425000 Validation Accuracy: 0.495200 \n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.2739 train Accuracy: 0.500000 Validation Accuracy: 0.483200 \n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.2764 train Accuracy: 0.500000 Validation Accuracy: 0.510200 \n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.2646 train Accuracy: 0.475000 Validation Accuracy: 0.482600 \n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.2006 train Accuracy: 0.650000 Validation Accuracy: 0.507000 \n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.2654 train Accuracy: 0.575000 Validation Accuracy: 0.500600 \n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.2821 train Accuracy: 0.575000 Validation Accuracy: 0.509400 \n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.1731 train Accuracy: 0.600000 Validation Accuracy: 0.547600 \n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.1657 train Accuracy: 0.500000 Validation Accuracy: 0.530600 \n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.0416 train Accuracy: 0.700000 Validation Accuracy: 0.546400 \n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.1202 train Accuracy: 0.650000 Validation Accuracy: 0.544000 \n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.1462 train Accuracy: 0.575000 Validation Accuracy: 0.527600 \n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.0062 train Accuracy: 0.625000 Validation Accuracy: 0.575200 \n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.0935 train Accuracy: 0.575000 Validation Accuracy: 0.519400 \n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.9642 train Accuracy: 0.625000 Validation Accuracy: 0.581600 \n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.0368 train Accuracy: 0.700000 Validation Accuracy: 0.586400 \n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.0373 train Accuracy: 0.600000 Validation Accuracy: 0.581000 \n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.9187 train Accuracy: 0.675000 Validation Accuracy: 0.604600 \n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.9787 train Accuracy: 0.600000 Validation Accuracy: 0.578000 \n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.7870 train Accuracy: 0.700000 Validation Accuracy: 0.599000 \n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.0019 train Accuracy: 0.550000 Validation Accuracy: 0.588200 \n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.8440 train Accuracy: 0.750000 Validation Accuracy: 0.586600 \n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.8361 train Accuracy: 0.725000 Validation Accuracy: 0.606600 \n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.8288 train Accuracy: 0.675000 Validation Accuracy: 0.604600 \n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.7795 train Accuracy: 0.750000 Validation Accuracy: 0.612600 \n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.8851 train Accuracy: 0.750000 Validation Accuracy: 0.633200 \n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.8229 train Accuracy: 0.750000 Validation Accuracy: 0.610400 \n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.7975 train Accuracy: 0.750000 Validation Accuracy: 0.647600 \n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.8468 train Accuracy: 0.600000 Validation Accuracy: 0.620400 \n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.7315 train Accuracy: 0.800000 Validation Accuracy: 0.625600 \n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.8133 train Accuracy: 0.775000 Validation Accuracy: 0.650400 \n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.7254 train Accuracy: 0.825000 Validation Accuracy: 0.624600 \n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.7085 train Accuracy: 0.750000 Validation Accuracy: 0.641200 \n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.7123 train Accuracy: 0.675000 Validation Accuracy: 0.626000 \n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.6727 train Accuracy: 0.800000 Validation Accuracy: 0.636200 \n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.7494 train Accuracy: 0.700000 Validation Accuracy: 0.661600 \n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.6825 train Accuracy: 0.825000 Validation Accuracy: 0.641400 \n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.6899 train Accuracy: 0.700000 Validation Accuracy: 0.655200 \n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.6636 train Accuracy: 0.750000 Validation Accuracy: 0.644200 \n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.5868 train Accuracy: 0.850000 Validation Accuracy: 0.643800 \n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.6986 train Accuracy: 0.775000 Validation Accuracy: 0.664000 \n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.6583 train Accuracy: 0.825000 Validation Accuracy: 0.660000 \n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.7084 train Accuracy: 0.775000 Validation Accuracy: 0.672200 \n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.6694 train Accuracy: 0.700000 Validation Accuracy: 0.662400 \n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.4920 train Accuracy: 0.900000 Validation Accuracy: 0.675000 \n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.7038 train Accuracy: 0.750000 Validation Accuracy: 0.663800 \n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.5632 train Accuracy: 0.875000 Validation Accuracy: 0.662000 \n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.6466 train Accuracy: 0.800000 Validation Accuracy: 0.685800 \n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.6741 train Accuracy: 0.725000 Validation Accuracy: 0.674200 \n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.5303 train Accuracy: 0.900000 Validation Accuracy: 0.668200 \n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.6349 train Accuracy: 0.800000 Validation Accuracy: 0.696000 \n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.7163 train Accuracy: 0.775000 Validation Accuracy: 0.644400 \n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.6195 train Accuracy: 0.800000 Validation Accuracy: 0.686000 \n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.5855 train Accuracy: 0.775000 Validation Accuracy: 0.677400 \n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.4882 train Accuracy: 0.875000 Validation Accuracy: 0.697000 \n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.5909 train Accuracy: 0.825000 Validation Accuracy: 0.691200 \n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.5203 train Accuracy: 0.850000 Validation Accuracy: 0.688600 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.5520 train Accuracy: 0.800000 Validation Accuracy: 0.693400 \n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.6171 train Accuracy: 0.775000 Validation Accuracy: 0.702200 \n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.4362 train Accuracy: 0.875000 Validation Accuracy: 0.709400 \n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.4932 train Accuracy: 0.825000 Validation Accuracy: 0.707400 \n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.4947 train Accuracy: 0.825000 Validation Accuracy: 0.703400 \n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.5662 train Accuracy: 0.800000 Validation Accuracy: 0.688600 \n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.5482 train Accuracy: 0.800000 Validation Accuracy: 0.665800 \n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.4288 train Accuracy: 0.875000 Validation Accuracy: 0.694000 \n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.5216 train Accuracy: 0.900000 Validation Accuracy: 0.696400 \n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.4929 train Accuracy: 0.925000 Validation Accuracy: 0.684200 \n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.5035 train Accuracy: 0.825000 Validation Accuracy: 0.697800 \n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.5401 train Accuracy: 0.850000 Validation Accuracy: 0.712000 \n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.3552 train Accuracy: 0.925000 Validation Accuracy: 0.713000 \n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.5035 train Accuracy: 0.850000 Validation Accuracy: 0.717600 \n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.4573 train Accuracy: 0.875000 Validation Accuracy: 0.697000 \n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.5037 train Accuracy: 0.850000 Validation Accuracy: 0.712800 \n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.5248 train Accuracy: 0.775000 Validation Accuracy: 0.714000 \n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.3719 train Accuracy: 0.975000 Validation Accuracy: 0.718000 \n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.4755 train Accuracy: 0.850000 Validation Accuracy: 0.727400 \n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.4275 train Accuracy: 0.925000 Validation Accuracy: 0.689400 \n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.4304 train Accuracy: 0.875000 Validation Accuracy: 0.730800 \n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.5061 train Accuracy: 0.825000 Validation Accuracy: 0.711600 \n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.3070 train Accuracy: 0.975000 Validation Accuracy: 0.723600 \n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.4045 train Accuracy: 0.900000 Validation Accuracy: 0.731200 \n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.3773 train Accuracy: 0.900000 Validation Accuracy: 0.720200 \n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.4563 train Accuracy: 0.825000 Validation Accuracy: 0.730400 \n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.4503 train Accuracy: 0.825000 Validation Accuracy: 0.722600 \n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.3308 train Accuracy: 0.925000 Validation Accuracy: 0.721600 \n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.3759 train Accuracy: 0.925000 Validation Accuracy: 0.731400 \n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.3037 train Accuracy: 0.950000 Validation Accuracy: 0.724000 \n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.4431 train Accuracy: 0.900000 Validation Accuracy: 0.722600 \n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.4523 train Accuracy: 0.800000 Validation Accuracy: 0.727600 \n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.3364 train Accuracy: 0.950000 Validation Accuracy: 0.727400 \n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.3556 train Accuracy: 0.950000 Validation Accuracy: 0.743400 \n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.3613 train Accuracy: 0.875000 Validation Accuracy: 0.732000 \n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.4019 train Accuracy: 0.875000 Validation Accuracy: 0.731600 \n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.3695 train Accuracy: 0.825000 Validation Accuracy: 0.726200 \n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.2992 train Accuracy: 0.950000 Validation Accuracy: 0.741200 \n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.3584 train Accuracy: 0.900000 Validation Accuracy: 0.731800 \n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.2913 train Accuracy: 0.925000 Validation Accuracy: 0.734800 \n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.4030 train Accuracy: 0.850000 Validation Accuracy: 0.740800 \n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.4176 train Accuracy: 0.800000 Validation Accuracy: 0.731200 \n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.2720 train Accuracy: 0.950000 Validation Accuracy: 0.745800 \n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.3235 train Accuracy: 0.925000 Validation Accuracy: 0.739800 \n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.2440 train Accuracy: 0.950000 Validation Accuracy: 0.739800 \n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.3800 train Accuracy: 0.875000 Validation Accuracy: 0.753000 \n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.4316 train Accuracy: 0.800000 Validation Accuracy: 0.734400 \n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.2765 train Accuracy: 0.950000 Validation Accuracy: 0.730400 \n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.3028 train Accuracy: 0.900000 Validation Accuracy: 0.745400 \n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.2906 train Accuracy: 0.950000 Validation Accuracy: 0.738400 \n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.3831 train Accuracy: 0.875000 Validation Accuracy: 0.745800 \n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.3931 train Accuracy: 0.825000 Validation Accuracy: 0.745800 \n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.2201 train Accuracy: 0.975000 Validation Accuracy: 0.749800 \n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.2800 train Accuracy: 0.950000 Validation Accuracy: 0.746000 \n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.2790 train Accuracy: 0.950000 Validation Accuracy: 0.740800 \n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.3566 train Accuracy: 0.900000 Validation Accuracy: 0.736400 \n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.3674 train Accuracy: 0.875000 Validation Accuracy: 0.742200 \n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.2361 train Accuracy: 0.975000 Validation Accuracy: 0.747400 \n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.2647 train Accuracy: 0.975000 Validation Accuracy: 0.745600 \n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.2448 train Accuracy: 0.975000 Validation Accuracy: 0.737400 \n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.3633 train Accuracy: 0.875000 Validation Accuracy: 0.747600 \n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.3906 train Accuracy: 0.850000 Validation Accuracy: 0.741200 \n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.2215 train Accuracy: 0.975000 Validation Accuracy: 0.752800 \n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.2997 train Accuracy: 0.900000 Validation Accuracy: 0.750400 \n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.2123 train Accuracy: 1.000000 Validation Accuracy: 0.753600 \n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.3693 train Accuracy: 0.900000 Validation Accuracy: 0.738200 \n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.3386 train Accuracy: 0.900000 Validation Accuracy: 0.728600 \n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.2063 train Accuracy: 0.975000 Validation Accuracy: 0.760000 \n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.2302 train Accuracy: 0.975000 Validation Accuracy: 0.763600 \n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.1830 train Accuracy: 0.975000 Validation Accuracy: 0.745600 \n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.3276 train Accuracy: 0.900000 Validation Accuracy: 0.758400 \n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.3422 train Accuracy: 0.850000 Validation Accuracy: 0.725000 \n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.2359 train Accuracy: 1.000000 Validation Accuracy: 0.758400 \n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.2309 train Accuracy: 0.975000 Validation Accuracy: 0.755800 \n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.1760 train Accuracy: 0.975000 Validation Accuracy: 0.760200 \n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.3027 train Accuracy: 0.900000 Validation Accuracy: 0.738600 \n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.3067 train Accuracy: 0.925000 Validation Accuracy: 0.750200 \n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.2085 train Accuracy: 1.000000 Validation Accuracy: 0.755800 \n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.2599 train Accuracy: 0.900000 Validation Accuracy: 0.749800 \n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.1931 train Accuracy: 0.950000 Validation Accuracy: 0.743200 \n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.2985 train Accuracy: 0.900000 Validation Accuracy: 0.756200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.2649 train Accuracy: 0.900000 Validation Accuracy: 0.751800 \n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.1795 train Accuracy: 0.975000 Validation Accuracy: 0.755400 \n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.2092 train Accuracy: 0.975000 Validation Accuracy: 0.753600 \n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.1863 train Accuracy: 1.000000 Validation Accuracy: 0.750200 \n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.2381 train Accuracy: 0.950000 Validation Accuracy: 0.746200 \n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.2395 train Accuracy: 0.950000 Validation Accuracy: 0.749200 \n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.1979 train Accuracy: 0.950000 Validation Accuracy: 0.762400 \n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.2200 train Accuracy: 0.975000 Validation Accuracy: 0.750400 \n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.1576 train Accuracy: 1.000000 Validation Accuracy: 0.752200 \n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.2574 train Accuracy: 0.925000 Validation Accuracy: 0.754800 \n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.2796 train Accuracy: 0.925000 Validation Accuracy: 0.750200 \n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.1742 train Accuracy: 0.950000 Validation Accuracy: 0.757400 \n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.1772 train Accuracy: 0.950000 Validation Accuracy: 0.763800 \n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.1409 train Accuracy: 1.000000 Validation Accuracy: 0.762800 \n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.2824 train Accuracy: 0.925000 Validation Accuracy: 0.762800 \n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.2487 train Accuracy: 0.925000 Validation Accuracy: 0.734800 \n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.2219 train Accuracy: 0.975000 Validation Accuracy: 0.758400 \n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.2205 train Accuracy: 0.925000 Validation Accuracy: 0.761400 \n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.1603 train Accuracy: 1.000000 Validation Accuracy: 0.749000 \n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.2259 train Accuracy: 0.975000 Validation Accuracy: 0.757200 \n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.2395 train Accuracy: 0.900000 Validation Accuracy: 0.756000 \n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.1656 train Accuracy: 1.000000 Validation Accuracy: 0.747200 \n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.1814 train Accuracy: 0.950000 Validation Accuracy: 0.766600 \n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.1660 train Accuracy: 1.000000 Validation Accuracy: 0.749000 \n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2535 train Accuracy: 0.900000 Validation Accuracy: 0.764800 \n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.2703 train Accuracy: 0.900000 Validation Accuracy: 0.737800 \n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.1359 train Accuracy: 1.000000 Validation Accuracy: 0.753200 \n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.1625 train Accuracy: 0.975000 Validation Accuracy: 0.755600 \n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.1710 train Accuracy: 0.975000 Validation Accuracy: 0.757400 \n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.1930 train Accuracy: 0.950000 Validation Accuracy: 0.763000 \n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.2756 train Accuracy: 0.900000 Validation Accuracy: 0.742200 \n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.1540 train Accuracy: 1.000000 Validation Accuracy: 0.764200 \n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.1501 train Accuracy: 0.950000 Validation Accuracy: 0.767200 \n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.1541 train Accuracy: 0.975000 Validation Accuracy: 0.765800 \n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.2332 train Accuracy: 0.950000 Validation Accuracy: 0.765600 \n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.2079 train Accuracy: 0.925000 Validation Accuracy: 0.768600 \n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.1645 train Accuracy: 0.975000 Validation Accuracy: 0.754000 \n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.1597 train Accuracy: 0.950000 Validation Accuracy: 0.769600 \n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.1016 train Accuracy: 1.000000 Validation Accuracy: 0.763800 \n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.2046 train Accuracy: 0.950000 Validation Accuracy: 0.775800 \n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.2124 train Accuracy: 0.975000 Validation Accuracy: 0.757200 \n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.1196 train Accuracy: 1.000000 Validation Accuracy: 0.772200 \n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.1311 train Accuracy: 0.950000 Validation Accuracy: 0.768000 \n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.1090 train Accuracy: 1.000000 Validation Accuracy: 0.767400 \n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.1816 train Accuracy: 0.925000 Validation Accuracy: 0.771400 \n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.2183 train Accuracy: 0.950000 Validation Accuracy: 0.763600 \n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.1168 train Accuracy: 1.000000 Validation Accuracy: 0.763400 \n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.1838 train Accuracy: 0.975000 Validation Accuracy: 0.765400 \n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.1269 train Accuracy: 1.000000 Validation Accuracy: 0.760800 \n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.1671 train Accuracy: 0.975000 Validation Accuracy: 0.764800 \n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.1908 train Accuracy: 0.975000 Validation Accuracy: 0.765400 \n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.1483 train Accuracy: 0.975000 Validation Accuracy: 0.754600 \n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.1492 train Accuracy: 1.000000 Validation Accuracy: 0.773200 \n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.0954 train Accuracy: 1.000000 Validation Accuracy: 0.772800 \n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.1576 train Accuracy: 0.975000 Validation Accuracy: 0.773400 \n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.1933 train Accuracy: 0.925000 Validation Accuracy: 0.772600 \n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.1254 train Accuracy: 1.000000 Validation Accuracy: 0.763200 \n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.1635 train Accuracy: 0.975000 Validation Accuracy: 0.769000 \n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.1074 train Accuracy: 1.000000 Validation Accuracy: 0.765600 \n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.1471 train Accuracy: 1.000000 Validation Accuracy: 0.766400 \n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.1813 train Accuracy: 0.950000 Validation Accuracy: 0.758200 \n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.1227 train Accuracy: 1.000000 Validation Accuracy: 0.768600 \n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.1609 train Accuracy: 0.950000 Validation Accuracy: 0.753600 \n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.1020 train Accuracy: 1.000000 Validation Accuracy: 0.761000 \n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1828 train Accuracy: 1.000000 Validation Accuracy: 0.767600 \n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.2060 train Accuracy: 1.000000 Validation Accuracy: 0.752600 \n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.0920 train Accuracy: 1.000000 Validation Accuracy: 0.770400 \n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.1390 train Accuracy: 0.950000 Validation Accuracy: 0.774000 \n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.0996 train Accuracy: 1.000000 Validation Accuracy: 0.763000 \n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.1412 train Accuracy: 1.000000 Validation Accuracy: 0.767800 \n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.1736 train Accuracy: 1.000000 Validation Accuracy: 0.757000 \n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.1094 train Accuracy: 1.000000 Validation Accuracy: 0.766600 \n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.1290 train Accuracy: 0.975000 Validation Accuracy: 0.758800 \n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.0938 train Accuracy: 1.000000 Validation Accuracy: 0.772000 \n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.1553 train Accuracy: 1.000000 Validation Accuracy: 0.773200 \n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.2006 train Accuracy: 0.975000 Validation Accuracy: 0.739600 \n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.1511 train Accuracy: 0.975000 Validation Accuracy: 0.769400 \n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.1225 train Accuracy: 1.000000 Validation Accuracy: 0.774400 \n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.0922 train Accuracy: 1.000000 Validation Accuracy: 0.775000 \n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1338 train Accuracy: 1.000000 Validation Accuracy: 0.778200 \n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.1738 train Accuracy: 1.000000 Validation Accuracy: 0.769000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.1446 train Accuracy: 0.975000 Validation Accuracy: 0.778000 \n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.1209 train Accuracy: 0.975000 Validation Accuracy: 0.771600 \n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.1010 train Accuracy: 1.000000 Validation Accuracy: 0.771600 \n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.1398 train Accuracy: 1.000000 Validation Accuracy: 0.773800 \n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.1501 train Accuracy: 0.975000 Validation Accuracy: 0.779600 \n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.1039 train Accuracy: 1.000000 Validation Accuracy: 0.763600 \n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.1232 train Accuracy: 0.975000 Validation Accuracy: 0.767800 \n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.0884 train Accuracy: 1.000000 Validation Accuracy: 0.773800 \n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1138 train Accuracy: 1.000000 Validation Accuracy: 0.774000 \n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.1545 train Accuracy: 1.000000 Validation Accuracy: 0.776000 \n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0961 train Accuracy: 1.000000 Validation Accuracy: 0.782600 \n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.1145 train Accuracy: 0.975000 Validation Accuracy: 0.771600 \n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.0831 train Accuracy: 1.000000 Validation Accuracy: 0.787000 \n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.1059 train Accuracy: 1.000000 Validation Accuracy: 0.776400 \n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.1425 train Accuracy: 0.975000 Validation Accuracy: 0.775600 \n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.1120 train Accuracy: 0.975000 Validation Accuracy: 0.756000 \n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.0953 train Accuracy: 0.975000 Validation Accuracy: 0.778600 \n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.0745 train Accuracy: 1.000000 Validation Accuracy: 0.772200 \n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.1402 train Accuracy: 0.975000 Validation Accuracy: 0.773000 \n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.1551 train Accuracy: 0.975000 Validation Accuracy: 0.767800 \n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.1159 train Accuracy: 0.975000 Validation Accuracy: 0.769400 \n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.1110 train Accuracy: 0.975000 Validation Accuracy: 0.780400 \n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.0748 train Accuracy: 1.000000 Validation Accuracy: 0.775400 \n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1023 train Accuracy: 1.000000 Validation Accuracy: 0.773400 \n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.1192 train Accuracy: 0.975000 Validation Accuracy: 0.776000 \n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.0963 train Accuracy: 0.975000 Validation Accuracy: 0.781000 \n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.0925 train Accuracy: 1.000000 Validation Accuracy: 0.779400 \n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.0647 train Accuracy: 1.000000 Validation Accuracy: 0.781200 \n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.1206 train Accuracy: 1.000000 Validation Accuracy: 0.777400 \n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.1430 train Accuracy: 1.000000 Validation Accuracy: 0.777000 \n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.0835 train Accuracy: 1.000000 Validation Accuracy: 0.778200 \n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.1044 train Accuracy: 0.975000 Validation Accuracy: 0.778000 \n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.0845 train Accuracy: 1.000000 Validation Accuracy: 0.772000 \n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.1349 train Accuracy: 0.975000 Validation Accuracy: 0.767800 \n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.1444 train Accuracy: 0.975000 Validation Accuracy: 0.781400 \n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.0835 train Accuracy: 1.000000 Validation Accuracy: 0.782800 \n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.1021 train Accuracy: 0.975000 Validation Accuracy: 0.776200 \n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.0626 train Accuracy: 1.000000 Validation Accuracy: 0.773800 \n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.0962 train Accuracy: 1.000000 Validation Accuracy: 0.786000 \n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.1169 train Accuracy: 1.000000 Validation Accuracy: 0.784200 \n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0784 train Accuracy: 1.000000 Validation Accuracy: 0.783400 \n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.0985 train Accuracy: 1.000000 Validation Accuracy: 0.783600 \n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.0738 train Accuracy: 1.000000 Validation Accuracy: 0.777200 \n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1236 train Accuracy: 0.975000 Validation Accuracy: 0.777600 \n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.1092 train Accuracy: 1.000000 Validation Accuracy: 0.784200 \n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.0677 train Accuracy: 1.000000 Validation Accuracy: 0.781800 \n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.0862 train Accuracy: 1.000000 Validation Accuracy: 0.776200 \n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.0747 train Accuracy: 1.000000 Validation Accuracy: 0.778200 \n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.0867 train Accuracy: 1.000000 Validation Accuracy: 0.778600 \n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.1376 train Accuracy: 0.975000 Validation Accuracy: 0.775800 \n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.0924 train Accuracy: 1.000000 Validation Accuracy: 0.781600 \n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.0970 train Accuracy: 1.000000 Validation Accuracy: 0.778200 \n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0828 train Accuracy: 0.975000 Validation Accuracy: 0.776800 \n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.0935 train Accuracy: 1.000000 Validation Accuracy: 0.778200 \n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.1231 train Accuracy: 1.000000 Validation Accuracy: 0.777200 \n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0713 train Accuracy: 1.000000 Validation Accuracy: 0.782200 \n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.0783 train Accuracy: 0.975000 Validation Accuracy: 0.783800 \n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.0672 train Accuracy: 1.000000 Validation Accuracy: 0.781600 \n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.1113 train Accuracy: 1.000000 Validation Accuracy: 0.779400 \n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.1074 train Accuracy: 1.000000 Validation Accuracy: 0.772000 \n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.0786 train Accuracy: 1.000000 Validation Accuracy: 0.784000 \n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.0820 train Accuracy: 1.000000 Validation Accuracy: 0.768600 \n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.0536 train Accuracy: 1.000000 Validation Accuracy: 0.786000 \n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0929 train Accuracy: 1.000000 Validation Accuracy: 0.782600 \n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.1112 train Accuracy: 1.000000 Validation Accuracy: 0.785200 \n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0868 train Accuracy: 1.000000 Validation Accuracy: 0.774000 \n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.0809 train Accuracy: 1.000000 Validation Accuracy: 0.785000 \n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0466 train Accuracy: 1.000000 Validation Accuracy: 0.775400 \n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.0933 train Accuracy: 1.000000 Validation Accuracy: 0.784200 \n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.0877 train Accuracy: 1.000000 Validation Accuracy: 0.780800 \n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0737 train Accuracy: 1.000000 Validation Accuracy: 0.771200 \n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0682 train Accuracy: 1.000000 Validation Accuracy: 0.769000 \n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.0629 train Accuracy: 1.000000 Validation Accuracy: 0.776600 \n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1136 train Accuracy: 0.975000 Validation Accuracy: 0.783200 \n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.1017 train Accuracy: 1.000000 Validation Accuracy: 0.790400 \n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0893 train Accuracy: 0.975000 Validation Accuracy: 0.764000 \n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.0680 train Accuracy: 1.000000 Validation Accuracy: 0.790200 \n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.0434 train Accuracy: 1.000000 Validation Accuracy: 0.792400 \n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0786 train Accuracy: 1.000000 Validation Accuracy: 0.780800 \n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.1120 train Accuracy: 1.000000 Validation Accuracy: 0.782000 \n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.0667 train Accuracy: 1.000000 Validation Accuracy: 0.769600 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.0633 train Accuracy: 1.000000 Validation Accuracy: 0.790000 \n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0468 train Accuracy: 1.000000 Validation Accuracy: 0.787200 \n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0831 train Accuracy: 1.000000 Validation Accuracy: 0.778800 \n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.1143 train Accuracy: 1.000000 Validation Accuracy: 0.782600 \n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0504 train Accuracy: 1.000000 Validation Accuracy: 0.779800 \n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.0716 train Accuracy: 1.000000 Validation Accuracy: 0.777600 \n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.0592 train Accuracy: 1.000000 Validation Accuracy: 0.784000 \n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1528 train Accuracy: 0.950000 Validation Accuracy: 0.776400 \n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.1051 train Accuracy: 1.000000 Validation Accuracy: 0.789800 \n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0773 train Accuracy: 1.000000 Validation Accuracy: 0.768800 \n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.0500 train Accuracy: 1.000000 Validation Accuracy: 0.793000 \n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0702 train Accuracy: 1.000000 Validation Accuracy: 0.779400 \n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.1040 train Accuracy: 1.000000 Validation Accuracy: 0.782000 \n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.0975 train Accuracy: 1.000000 Validation Accuracy: 0.786600 \n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0589 train Accuracy: 1.000000 Validation Accuracy: 0.785600 \n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.0479 train Accuracy: 1.000000 Validation Accuracy: 0.783800 \n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.0534 train Accuracy: 1.000000 Validation Accuracy: 0.781400 \n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0877 train Accuracy: 1.000000 Validation Accuracy: 0.783600 \n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.0841 train Accuracy: 1.000000 Validation Accuracy: 0.790800 \n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.0494 train Accuracy: 1.000000 Validation Accuracy: 0.782800 \n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0809 train Accuracy: 1.000000 Validation Accuracy: 0.779800 \n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0804 train Accuracy: 0.975000 Validation Accuracy: 0.786000 \n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0725 train Accuracy: 1.000000 Validation Accuracy: 0.789400 \n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.0875 train Accuracy: 1.000000 Validation Accuracy: 0.766600 \n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0878 train Accuracy: 1.000000 Validation Accuracy: 0.789800 \n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.0730 train Accuracy: 1.000000 Validation Accuracy: 0.779400 \n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.0455 train Accuracy: 1.000000 Validation Accuracy: 0.797000 \n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0772 train Accuracy: 1.000000 Validation Accuracy: 0.783800 \n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.0684 train Accuracy: 1.000000 Validation Accuracy: 0.786600 \n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.0623 train Accuracy: 1.000000 Validation Accuracy: 0.782800 \n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.0579 train Accuracy: 1.000000 Validation Accuracy: 0.788000 \n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0577 train Accuracy: 1.000000 Validation Accuracy: 0.782600 \n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0631 train Accuracy: 1.000000 Validation Accuracy: 0.794400 \n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.0841 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.0498 train Accuracy: 1.000000 Validation Accuracy: 0.789600 \n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0732 train Accuracy: 1.000000 Validation Accuracy: 0.772200 \n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0659 train Accuracy: 1.000000 Validation Accuracy: 0.793400 \n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0751 train Accuracy: 1.000000 Validation Accuracy: 0.801800 \n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.0744 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0762 train Accuracy: 0.975000 Validation Accuracy: 0.798400 \n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.0604 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0411 train Accuracy: 1.000000 Validation Accuracy: 0.795600 \n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0734 train Accuracy: 1.000000 Validation Accuracy: 0.787400 \n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.0767 train Accuracy: 1.000000 Validation Accuracy: 0.787000 \n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0859 train Accuracy: 0.975000 Validation Accuracy: 0.770200 \n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.0797 train Accuracy: 1.000000 Validation Accuracy: 0.785600 \n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.0652 train Accuracy: 1.000000 Validation Accuracy: 0.787800 \n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.1196 train Accuracy: 0.975000 Validation Accuracy: 0.775800 \n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.0806 train Accuracy: 1.000000 Validation Accuracy: 0.789400 \n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0602 train Accuracy: 1.000000 Validation Accuracy: 0.786400 \n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0817 train Accuracy: 1.000000 Validation Accuracy: 0.785600 \n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.0412 train Accuracy: 1.000000 Validation Accuracy: 0.793200 \n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0946 train Accuracy: 0.975000 Validation Accuracy: 0.783800 \n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0623 train Accuracy: 1.000000 Validation Accuracy: 0.781800 \n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0477 train Accuracy: 1.000000 Validation Accuracy: 0.783600 \n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0653 train Accuracy: 1.000000 Validation Accuracy: 0.775200 \n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.0471 train Accuracy: 1.000000 Validation Accuracy: 0.792200 \n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0649 train Accuracy: 1.000000 Validation Accuracy: 0.795200 \n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.0584 train Accuracy: 1.000000 Validation Accuracy: 0.792400 \n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0479 train Accuracy: 1.000000 Validation Accuracy: 0.792200 \n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0661 train Accuracy: 1.000000 Validation Accuracy: 0.788600 \n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0553 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0912 train Accuracy: 0.975000 Validation Accuracy: 0.788000 \n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0538 train Accuracy: 1.000000 Validation Accuracy: 0.801400 \n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0353 train Accuracy: 1.000000 Validation Accuracy: 0.801200 \n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.0454 train Accuracy: 1.000000 Validation Accuracy: 0.790200 \n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.0407 train Accuracy: 1.000000 Validation Accuracy: 0.789600 \n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0609 train Accuracy: 1.000000 Validation Accuracy: 0.794200 \n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.0571 train Accuracy: 1.000000 Validation Accuracy: 0.795800 \n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0477 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.0742 train Accuracy: 1.000000 Validation Accuracy: 0.788800 \n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.0453 train Accuracy: 1.000000 Validation Accuracy: 0.791200 \n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0749 train Accuracy: 1.000000 Validation Accuracy: 0.790600 \n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.0521 train Accuracy: 1.000000 Validation Accuracy: 0.793000 \n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0347 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0463 train Accuracy: 1.000000 Validation Accuracy: 0.791000 \n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0426 train Accuracy: 1.000000 Validation Accuracy: 0.796600 \n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0702 train Accuracy: 1.000000 Validation Accuracy: 0.790800 \n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.0611 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0348 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0502 train Accuracy: 1.000000 Validation Accuracy: 0.788000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.0455 train Accuracy: 1.000000 Validation Accuracy: 0.788000 \n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0525 train Accuracy: 1.000000 Validation Accuracy: 0.788800 \n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0501 train Accuracy: 1.000000 Validation Accuracy: 0.795000 \n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.0346 train Accuracy: 1.000000 Validation Accuracy: 0.788200 \n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0474 train Accuracy: 1.000000 Validation Accuracy: 0.787600 \n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0410 train Accuracy: 1.000000 Validation Accuracy: 0.795000 \n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0509 train Accuracy: 1.000000 Validation Accuracy: 0.795600 \n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.0594 train Accuracy: 1.000000 Validation Accuracy: 0.793000 \n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.0444 train Accuracy: 1.000000 Validation Accuracy: 0.783200 \n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0526 train Accuracy: 1.000000 Validation Accuracy: 0.784600 \n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0331 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0598 train Accuracy: 1.000000 Validation Accuracy: 0.790000 \n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.0546 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0414 train Accuracy: 1.000000 Validation Accuracy: 0.791400 \n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0527 train Accuracy: 1.000000 Validation Accuracy: 0.793200 \n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0371 train Accuracy: 1.000000 Validation Accuracy: 0.789400 \n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0505 train Accuracy: 1.000000 Validation Accuracy: 0.791200 \n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0698 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.0354 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.0509 train Accuracy: 1.000000 Validation Accuracy: 0.798600 \n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0252 train Accuracy: 1.000000 Validation Accuracy: 0.799600 \n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0503 train Accuracy: 1.000000 Validation Accuracy: 0.794800 \n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0507 train Accuracy: 1.000000 Validation Accuracy: 0.797200 \n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0487 train Accuracy: 1.000000 Validation Accuracy: 0.786800 \n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0746 train Accuracy: 1.000000 Validation Accuracy: 0.776800 \n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0354 train Accuracy: 1.000000 Validation Accuracy: 0.798600 \n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0597 train Accuracy: 1.000000 Validation Accuracy: 0.792200 \n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.0694 train Accuracy: 1.000000 Validation Accuracy: 0.802000 \n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0285 train Accuracy: 1.000000 Validation Accuracy: 0.794200 \n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.0599 train Accuracy: 1.000000 Validation Accuracy: 0.789600 \n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0303 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0395 train Accuracy: 1.000000 Validation Accuracy: 0.794800 \n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.0308 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0348 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.0443 train Accuracy: 1.000000 Validation Accuracy: 0.784400 \n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0488 train Accuracy: 0.975000 Validation Accuracy: 0.787000 \n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0384 train Accuracy: 1.000000 Validation Accuracy: 0.800000 \n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0451 train Accuracy: 1.000000 Validation Accuracy: 0.797200 \n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0338 train Accuracy: 1.000000 Validation Accuracy: 0.788000 \n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0409 train Accuracy: 1.000000 Validation Accuracy: 0.780000 \n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0309 train Accuracy: 1.000000 Validation Accuracy: 0.792200 \n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0495 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0457 train Accuracy: 1.000000 Validation Accuracy: 0.787800 \n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0416 train Accuracy: 1.000000 Validation Accuracy: 0.794600 \n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0474 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.0382 train Accuracy: 1.000000 Validation Accuracy: 0.795800 \n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0466 train Accuracy: 1.000000 Validation Accuracy: 0.803000 \n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.0494 train Accuracy: 1.000000 Validation Accuracy: 0.804400 \n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.0322 train Accuracy: 1.000000 Validation Accuracy: 0.802000 \n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0343 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0313 train Accuracy: 1.000000 Validation Accuracy: 0.794600 \n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0379 train Accuracy: 1.000000 Validation Accuracy: 0.803200 \n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.0339 train Accuracy: 1.000000 Validation Accuracy: 0.798000 \n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0365 train Accuracy: 1.000000 Validation Accuracy: 0.792000 \n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0414 train Accuracy: 1.000000 Validation Accuracy: 0.785200 \n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0300 train Accuracy: 1.000000 Validation Accuracy: 0.791800 \n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0484 train Accuracy: 1.000000 Validation Accuracy: 0.799800 \n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.0344 train Accuracy: 1.000000 Validation Accuracy: 0.790600 \n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0376 train Accuracy: 1.000000 Validation Accuracy: 0.803800 \n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0397 train Accuracy: 1.000000 Validation Accuracy: 0.781000 \n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0448 train Accuracy: 1.000000 Validation Accuracy: 0.792600 \n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0442 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0405 train Accuracy: 1.000000 Validation Accuracy: 0.796200 \n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0265 train Accuracy: 1.000000 Validation Accuracy: 0.794200 \n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0435 train Accuracy: 1.000000 Validation Accuracy: 0.779800 \n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0301 train Accuracy: 1.000000 Validation Accuracy: 0.785400 \n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0360 train Accuracy: 1.000000 Validation Accuracy: 0.797600 \n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0601 train Accuracy: 1.000000 Validation Accuracy: 0.789200 \n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0342 train Accuracy: 1.000000 Validation Accuracy: 0.799800 \n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0549 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0309 train Accuracy: 1.000000 Validation Accuracy: 0.809600 \n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0462 train Accuracy: 1.000000 Validation Accuracy: 0.800200 \n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.0363 train Accuracy: 1.000000 Validation Accuracy: 0.806000 \n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0374 train Accuracy: 1.000000 Validation Accuracy: 0.796200 \n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0461 train Accuracy: 1.000000 Validation Accuracy: 0.779200 \n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.0345 train Accuracy: 1.000000 Validation Accuracy: 0.795000 \n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0343 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0326 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0236 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.0401 train Accuracy: 1.000000 Validation Accuracy: 0.793600 \n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0305 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0431 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.0364 train Accuracy: 1.000000 Validation Accuracy: 0.783200 \n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0230 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0391 train Accuracy: 1.000000 Validation Accuracy: 0.785000 \n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.0316 train Accuracy: 1.000000 Validation Accuracy: 0.794000 \n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0560 train Accuracy: 1.000000 Validation Accuracy: 0.796400 \n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.0260 train Accuracy: 1.000000 Validation Accuracy: 0.799200 \n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0240 train Accuracy: 1.000000 Validation Accuracy: 0.795200 \n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0322 train Accuracy: 1.000000 Validation Accuracy: 0.804600 \n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0225 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0415 train Accuracy: 1.000000 Validation Accuracy: 0.802400 \n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0298 train Accuracy: 1.000000 Validation Accuracy: 0.793600 \n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0272 train Accuracy: 1.000000 Validation Accuracy: 0.803200 \n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.0364 train Accuracy: 1.000000 Validation Accuracy: 0.799200 \n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0241 train Accuracy: 1.000000 Validation Accuracy: 0.793600 \n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.0421 train Accuracy: 1.000000 Validation Accuracy: 0.805400 \n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.0302 train Accuracy: 1.000000 Validation Accuracy: 0.783400 \n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.0318 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.0393 train Accuracy: 1.000000 Validation Accuracy: 0.796400 \n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.0346 train Accuracy: 1.000000 Validation Accuracy: 0.785400 \n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.0490 train Accuracy: 1.000000 Validation Accuracy: 0.800400 \n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.0321 train Accuracy: 1.000000 Validation Accuracy: 0.800800 \n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.0199 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.0291 train Accuracy: 1.000000 Validation Accuracy: 0.780400 \n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.0154 train Accuracy: 1.000000 Validation Accuracy: 0.801400 \n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.0430 train Accuracy: 1.000000 Validation Accuracy: 0.797200 \n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.0301 train Accuracy: 1.000000 Validation Accuracy: 0.797000 \n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.0262 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.0432 train Accuracy: 1.000000 Validation Accuracy: 0.789200 \n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.0259 train Accuracy: 1.000000 Validation Accuracy: 0.802800 \n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.0338 train Accuracy: 1.000000 Validation Accuracy: 0.799800 \n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.0367 train Accuracy: 1.000000 Validation Accuracy: 0.805600 \n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.0424 train Accuracy: 1.000000 Validation Accuracy: 0.782600 \n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.0304 train Accuracy: 1.000000 Validation Accuracy: 0.789400 \n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.0228 train Accuracy: 1.000000 Validation Accuracy: 0.790800 \n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.0347 train Accuracy: 1.000000 Validation Accuracy: 0.799800 \n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.0248 train Accuracy: 1.000000 Validation Accuracy: 0.800600 \n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.0212 train Accuracy: 1.000000 Validation Accuracy: 0.791800 \n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.0281 train Accuracy: 1.000000 Validation Accuracy: 0.793000 \n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.0220 train Accuracy: 1.000000 Validation Accuracy: 0.804000 \n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.0469 train Accuracy: 1.000000 Validation Accuracy: 0.800400 \n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.0431 train Accuracy: 1.000000 Validation Accuracy: 0.786600 \n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.0291 train Accuracy: 1.000000 Validation Accuracy: 0.792600 \n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.0359 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.0237 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.0600 train Accuracy: 0.975000 Validation Accuracy: 0.798200 \n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.0302 train Accuracy: 1.000000 Validation Accuracy: 0.801600 \n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.0315 train Accuracy: 1.000000 Validation Accuracy: 0.805600 \n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.0305 train Accuracy: 1.000000 Validation Accuracy: 0.789800 \n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.0328 train Accuracy: 1.000000 Validation Accuracy: 0.801000 \n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.0350 train Accuracy: 1.000000 Validation Accuracy: 0.806000 \n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.0344 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.0196 train Accuracy: 1.000000 Validation Accuracy: 0.793800 \n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.0225 train Accuracy: 1.000000 Validation Accuracy: 0.788400 \n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.0261 train Accuracy: 1.000000 Validation Accuracy: 0.803200 \n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.0398 train Accuracy: 1.000000 Validation Accuracy: 0.808000 \n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.0283 train Accuracy: 1.000000 Validation Accuracy: 0.799400 \n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.0185 train Accuracy: 1.000000 Validation Accuracy: 0.805400 \n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.0292 train Accuracy: 1.000000 Validation Accuracy: 0.800400 \n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.0227 train Accuracy: 1.000000 Validation Accuracy: 0.807200 \n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.0305 train Accuracy: 1.000000 Validation Accuracy: 0.802400 \n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.0275 train Accuracy: 1.000000 Validation Accuracy: 0.801200 \n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.0208 train Accuracy: 1.000000 Validation Accuracy: 0.804800 \n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.0473 train Accuracy: 1.000000 Validation Accuracy: 0.793200 \n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.0275 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.0431 train Accuracy: 1.000000 Validation Accuracy: 0.800400 \n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.0352 train Accuracy: 1.000000 Validation Accuracy: 0.778800 \n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.0179 train Accuracy: 1.000000 Validation Accuracy: 0.793000 \n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.0376 train Accuracy: 1.000000 Validation Accuracy: 0.793400 \n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.0180 train Accuracy: 1.000000 Validation Accuracy: 0.798200 \n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.0440 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.0256 train Accuracy: 1.000000 Validation Accuracy: 0.801800 \n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.0199 train Accuracy: 1.000000 Validation Accuracy: 0.800600 \n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.0307 train Accuracy: 1.000000 Validation Accuracy: 0.799800 \n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.0269 train Accuracy: 1.000000 Validation Accuracy: 0.781400 \n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.0538 train Accuracy: 1.000000 Validation Accuracy: 0.788800 \n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.0301 train Accuracy: 1.000000 Validation Accuracy: 0.802800 \n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.0255 train Accuracy: 1.000000 Validation Accuracy: 0.804000 \n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.0382 train Accuracy: 1.000000 Validation Accuracy: 0.785000 \n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.0323 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.0247 train Accuracy: 1.000000 Validation Accuracy: 0.802600 \n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.0281 train Accuracy: 1.000000 Validation Accuracy: 0.791400 \n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.0243 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.0317 train Accuracy: 1.000000 Validation Accuracy: 0.798800 \n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.0149 train Accuracy: 1.000000 Validation Accuracy: 0.792800 \n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.0266 train Accuracy: 1.000000 Validation Accuracy: 0.799600 \n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.0226 train Accuracy: 1.000000 Validation Accuracy: 0.799200 \n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.0252 train Accuracy: 1.000000 Validation Accuracy: 0.802400 \n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.0254 train Accuracy: 1.000000 Validation Accuracy: 0.790400 \n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.0184 train Accuracy: 1.000000 Validation Accuracy: 0.802000 \n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.0261 train Accuracy: 1.000000 Validation Accuracy: 0.796600 \n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.0203 train Accuracy: 1.000000 Validation Accuracy: 0.801800 \n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.0169 train Accuracy: 1.000000 Validation Accuracy: 0.802600 \n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.0219 train Accuracy: 1.000000 Validation Accuracy: 0.795800 \n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.0101 train Accuracy: 1.000000 Validation Accuracy: 0.804000 \n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.0569 train Accuracy: 1.000000 Validation Accuracy: 0.785800 \n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.0231 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.0300 train Accuracy: 1.000000 Validation Accuracy: 0.800600 \n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.0230 train Accuracy: 1.000000 Validation Accuracy: 0.791000 \n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.0233 train Accuracy: 1.000000 Validation Accuracy: 0.790000 \n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.0222 train Accuracy: 1.000000 Validation Accuracy: 0.805000 \n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.0320 train Accuracy: 1.000000 Validation Accuracy: 0.792600 \n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.0174 train Accuracy: 1.000000 Validation Accuracy: 0.796600 \n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.0269 train Accuracy: 1.000000 Validation Accuracy: 0.786600 \n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.0225 train Accuracy: 1.000000 Validation Accuracy: 0.789400 \n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.0303 train Accuracy: 1.000000 Validation Accuracy: 0.803000 \n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.0338 train Accuracy: 1.000000 Validation Accuracy: 0.791600 \n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.0200 train Accuracy: 1.000000 Validation Accuracy: 0.796400 \n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.0236 train Accuracy: 1.000000 Validation Accuracy: 0.800600 \n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.0208 train Accuracy: 1.000000 Validation Accuracy: 0.803400 \n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.0346 train Accuracy: 1.000000 Validation Accuracy: 0.801400 \n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.0317 train Accuracy: 1.000000 Validation Accuracy: 0.800800 \n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.0239 train Accuracy: 1.000000 Validation Accuracy: 0.807000 \n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.0300 train Accuracy: 1.000000 Validation Accuracy: 0.801200 \n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.0183 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.0318 train Accuracy: 1.000000 Validation Accuracy: 0.795800 \n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.0304 train Accuracy: 1.000000 Validation Accuracy: 0.793200 \n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.0210 train Accuracy: 1.000000 Validation Accuracy: 0.802800 \n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.0221 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.0139 train Accuracy: 1.000000 Validation Accuracy: 0.799400 \n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.0396 train Accuracy: 1.000000 Validation Accuracy: 0.798200 \n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.0395 train Accuracy: 1.000000 Validation Accuracy: 0.791200 \n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.0213 train Accuracy: 1.000000 Validation Accuracy: 0.806200 \n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.0241 train Accuracy: 1.000000 Validation Accuracy: 0.778600 \n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.0161 train Accuracy: 1.000000 Validation Accuracy: 0.808600 \n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.0383 train Accuracy: 1.000000 Validation Accuracy: 0.796000 \n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.0304 train Accuracy: 1.000000 Validation Accuracy: 0.803000 \n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.0277 train Accuracy: 1.000000 Validation Accuracy: 0.804000 \n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.0286 train Accuracy: 1.000000 Validation Accuracy: 0.791800 \n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.0231 train Accuracy: 1.000000 Validation Accuracy: 0.790400 \n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.0340 train Accuracy: 1.000000 Validation Accuracy: 0.804400 \n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.0271 train Accuracy: 1.000000 Validation Accuracy: 0.804200 \n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.0185 train Accuracy: 1.000000 Validation Accuracy: 0.811000 \n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.0335 train Accuracy: 1.000000 Validation Accuracy: 0.785600 \n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.0176 train Accuracy: 1.000000 Validation Accuracy: 0.795800 \n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.0284 train Accuracy: 1.000000 Validation Accuracy: 0.799600 \n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.0241 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.0313 train Accuracy: 1.000000 Validation Accuracy: 0.799000 \n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.0442 train Accuracy: 1.000000 Validation Accuracy: 0.798600 \n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.0209 train Accuracy: 1.000000 Validation Accuracy: 0.799400 \n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.0291 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.0229 train Accuracy: 1.000000 Validation Accuracy: 0.805200 \n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.0176 train Accuracy: 1.000000 Validation Accuracy: 0.798600 \n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.0215 train Accuracy: 1.000000 Validation Accuracy: 0.797800 \n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.0246 train Accuracy: 1.000000 Validation Accuracy: 0.780200 \n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.0510 train Accuracy: 1.000000 Validation Accuracy: 0.792200 \n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.0327 train Accuracy: 1.000000 Validation Accuracy: 0.807000 \n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.0181 train Accuracy: 1.000000 Validation Accuracy: 0.795400 \n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.0202 train Accuracy: 1.000000 Validation Accuracy: 0.799400 \n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.0195 train Accuracy: 1.000000 Validation Accuracy: 0.803400 \n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.0203 train Accuracy: 1.000000 Validation Accuracy: 0.801200 \n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.0447 train Accuracy: 1.000000 Validation Accuracy: 0.807800 \n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.0246 train Accuracy: 1.000000 Validation Accuracy: 0.798400 \n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.0365 train Accuracy: 1.000000 Validation Accuracy: 0.780400 \n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.0154 train Accuracy: 1.000000 Validation Accuracy: 0.796600 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                #print(\"batch_features:\"+str(batch_features.shape))\n",
    "                #print(\"batch_labels:\"+str(batch_labels))\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03d5be5c88>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
